# ! pip install selenium

# !apt-get update 
# !apt install chromium-chromedriver

# from selenium import webdriver
# chrome_options = webdriver.ChromeOptions()
# chrome_options.add_argument('--headless')
# chrome_options.add_argument('--no-sandbox')
# chrome_options.add_argument('--disable-dev-shm-usage')
# driver = webdriver.Chrome('chromedriver',chrome_options=chrome_options)
# driver.get("https://twitter.com/TilburgU")
# # make sure to login to your Twitter account first!
# from bs4 import BeautifulSoup
# res = driver.page_source.encode('utf-8')
# soup = BeautifulSoup(res, "html.parser")
import pandas as pd
import numpy as np
# def search_tweets(query):
#     driver.get("https://twitter.com/search?q=" + query) 
#     tweets = []
#     last_height = 0
#     current_height = 0

#     iterator = 0
#     while True: 
#         iterator += 1
        
#         current_height = driver.execute_script('return document.body.scrollHeight')

#         if current_height == last_height: 
#             break

#         res = driver.page_source.encode('utf-8')
#         soup = BeautifulSoup(res, "html.parser")
#         data = soup.find(attrs={"data-testid": "primaryColumn"}).find_all(attrs={"data-testid":"tweet"})

#         for counter in range(len(data)):
#             tweet = data[counter].find_all(attrs={"dir":"auto"})

#             text = tweet[4].text
#             try:
#                 link = tweet[3]['href']
#             except:
#                 link = '/LINK-NOT-AVAILABLE'

#             tweet_data = {
#                             "tweet": text, 
#                             "link": "https://twitter.com/" + link
#                          }

#             if tweet_data not in tweets: 
#                 tweets.append(tweet_data)

#         driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')       
#         last_height = current_height
        
#         if (iterator == 2): break
        
#     df = pd.DataFrame(tweets)
#     df.to_csv("search_tweets.csv", index=False)

# collected_tweets = search_tweets('("Tilburg University" OR "Tilburg Universiteit")')# Run search
# collected_tweets
import time
from time import sleep
# from selenium import webdriver
# from selenium.webdriver.common.keys import Keys
# class SeleniumClient(object):
#     def __init__(self):
#         #Initialization method. 
#         self.chrome_options = webdriver.ChromeOptions()
#         self.chrome_options.add_argument('--headless')
#         self.chrome_options.add_argument('--no-sandbox')
#         self.chrome_options.add_argument('--disable-setuid-sandbox')

#         # you need to provide the path of chromdriver in your system
#         self.browser = webdriver.Chrome('chromedriver', options=self.chrome_options)

#         self.base_url = 'https://twitter.com/search?q='

#     def get_tweets(self, query):
#         ''' 
#         Function to fetch tweets. 
#         '''
        
#         self.browser.get(self.base_url+query)
#         time.sleep(2)

#         body = self.browser.find_element_by_tag_name('body')

#         for _ in range(3000):
#             body.send_keys(Keys.PAGE_DOWN)
#             time.sleep(0.3)

#         timeline = self.browser.find_element_by_id('timeline')
#         tweet_nodes = timeline.find_elements_by_css_selector('.tweet-text')

#         return pd.DataFrame({'tweets': [tweet_node.text for tweet_node in tweet_nodes]})

        
# selenium_client = SeleniumClient()

# tweets_df = selenium_client.get_tweets('AI and Deep learning')
# Getting tweets from twitter
import tweepy
from tweepy import OAuthHandler
class TwitterClient(object): 
    def __init__(self):
        # Access Credentials 
        consumer_key = 'GXiPV0P6hJenfSfOHwHi3qfAR'
        consumer_secret = 'AAGUf3AkwHHtICARCBuoirCnBtfPQEquGsQH7HVPOnPrq6Mu5G'
        access_token = '1520529519374786560-ynmwaEA5FEelmUsO0Etcx89PEpnSKF'
        access_token_secret = '2yNBlLypZEF1g4G6DM8S6bTHrvYHAhWWCOuLlwcDe20wq'
        try: 
            # OAuthHandler object 
            auth = OAuthHandler(consumer_key, consumer_secret) 
            # set access token and secret 
            auth.set_access_token(access_token, access_token_secret) 
            # create tweepy API object to fetch tweets 
            self.api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)
            
        except tweepy.TweepError as e:
            print(f"Error: Twitter Authentication Failed - \n{str(e)}") 

    # Function to fetch tweets
    def get_tweets(self, query, maxTweets = 1000): 
        # empty list to store parsed tweets 
        tweets = [] 
        sinceId = None
        max_id = -1
        tweetCount = 0
        tweetsPerQry = 100
        
        while tweetCount < maxTweets:
            try:
                if (max_id <= 0):
                    if (not sinceId):
                        new_tweets = self.api.search(q=query, count=tweetsPerQry)
                    else:
                        new_tweets = self.api.search(q=query, count=tweetsPerQry,
                                                since_id=sinceId)
                else:
                    if (not sinceId):
                        new_tweets = self.api.search(q=query, count=tweetsPerQry,
                                                max_id=str(max_id - 1))
                    else:
                        new_tweets = self.api.search(q=query, count=tweetsPerQry,
                                                max_id=str(max_id - 1),
                                                since_id=sinceId)
                if not new_tweets:
                    print("No more tweets found")
                    break
                    
                for tweet in new_tweets:
                    parsed_tweet = {} 
                    parsed_tweet['tweets'] = tweet.text 

                    # appending parsed tweet to tweets list 
                    if tweet.retweet_count > 0: 
                        # if tweet has retweets, ensure that it is appended only once 
                        if parsed_tweet not in tweets: 
                            tweets.append(parsed_tweet) 
                    else: 
                        tweets.append(parsed_tweet) 
                        
                tweetCount += len(new_tweets)
                print("Downloaded {0} tweets".format(tweetCount))
                max_id = new_tweets[-1].id

            except tweepy.TweepError as e:
                print("Tweepy error : " + str(e))
                break
        
        return pd.DataFrame(tweets)
selenium_client = TwitterClient()

tweets_df = selenium_client.get_tweets('environment')
tweets_df.head()
# Pre Processing 
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from textblob import TextBlob
def fetch_sentiment_using_SIA(text):
    sid = SentimentIntensityAnalyzer()
    polarity_scores = sid.polarity_scores(text)
    if polarity_scores['neg'] > polarity_scores['pos']:
        return 'negative'
    else:
        return 'positive'

def fetch_sentiment_using_textblob(text):
    analysis = TextBlob(text)
    # set sentiment 
    if analysis.sentiment.polarity > 0:
        return 'positive'
    elif analysis.sentiment.polarity < 0: 
        return 'negative'
    else:
        return 'neutral'
import re
def remove_pattern(text, pattern_regex):
    r = re.findall(pattern_regex, text)
    for i in r:
        text = re.sub(i, '', text)
    
    return text
# We are keeping cleaned tweets in a new column called 'tidy_tweets'
tweets_df['tidy_tweets'] = np.vectorize(remove_pattern)(tweets_df['tweets'], "@[\w]*: | *RT*")
cleaned_tweets = []

for index, row in tweets_df.iterrows():
    # Here we are filtering out all the words that contains link
    words_without_links = [word for word in row.tidy_tweets.split()        if 'http' not in word]
    cleaned_tweets.append(' '.join(words_without_links))

tweets_df['tidy_tweets'] = cleaned_tweets
tweets_df.drop_duplicates(subset=['tidy_tweets'], keep=False)
tweets_df['absolute_tidy_tweets'] = tweets_df['tidy_tweets'].str.replace("[^a-zA-Z# ]", "")
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords_set = set(stopwords.words("english"))
cleaned_tweets = []

for index, row in tweets_df.iterrows():
    
    # filerting out all the stopwords 
    words_without_stopwords = [word for word in row.absolute_tidy_tweets.split() if not word in stopwords_set]
    
    # finally creating tweets list of tuples containing stopwords(list) and sentimentType 
    cleaned_tweets.append(' '.join(words_without_stopwords))

tweets_df['absolute_tidy_tweets'] = cleaned_tweets
import nltk

nltk.download('all')
from nltk.stem import WordNetLemmatizer
# Tokenization
tokenized_tweet = tweets_df['absolute_tidy_tweets'].apply(lambda x: x.split())
# Finding Lemma for each word
word_lemmatizer = WordNetLemmatizer()
tokenized_tweet = tokenized_tweet.apply(lambda x: [word_lemmatizer.lemmatize(i) for i in x])
#joining words into sentences (from where they came from)
for i, tokens in enumerate(tokenized_tweet):
    tokenized_tweet[i] = ' '.join(tokens)

tweets_df['absolute_tidy_tweets'] = tokenized_tweet
tweets_df
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
# BOW features
bow_word_vectorizer = CountVectorizer(max_df=0.90, min_df=2, stop_words='english', max_features=50)
# bag-of-words feature matrix
bow_word_feature = bow_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])

# TF-IDF features
tfidf_word_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, stop_words='english', max_features=50)
# TF-IDF feature matrix
tfidf_word_feature = tfidf_word_vectorizer.fit_transform(tweets_df['absolute_tidy_tweets'])
bow_word_feature
tweets_df
for row in tweets_df.itertuples():
    tweet = tweets_df.at[row[0], 'absolute_tidy_tweets']
    result=fetch_sentiment_using_textblob(tweet)
    tweets_df.at[row[0], 'Sentiment']=result
tweets_df['Sentiment'].unique()
tweets_df.head(100)
target_variable = tweets_df['Sentiment'].apply(lambda x: 0 if x=='negative' else 1 if x=='positive' else -1 )

# Modeling
## Gaussian NB
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn import metrics

def naive_model(X_train, X_test, y_train, y_test):
    naive_classifier = GaussianNB()
    naive_classifier.fit(X_train.toarray(), y_train)
    import joblib
    from joblib import dump

    # dump the pipeline model
    dump(naive_classifier, filename="text_classification.joblib")
    # predictions over test set
    predictions = naive_classifier.predict(X_test.toarray())
    
    # calculating f1 score
    accuracy=metrics.accuracy_score(predictions,y_test)
    print(f'Accuracy Score - {accuracy}')
X_train, X_test, y_train, y_test = train_test_split(bow_word_feature, target_variable, test_size=0.3, random_state=870)
naive_model(X_train, X_test, y_train, y_test)
X_train
X_test
X_train, X_test, y_train, y_test = train_test_split(tfidf_word_feature, target_variable, test_size=0.3, random_state=870)
naive_model(X_train, X_test, y_train, y_test)

from google.colab import files
files.download('text_classification.joblib') 
from tensorflow.keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.optimizers import RMSprop
%matplotlib inline

import warnings
warnings.filterwarnings("ignore")
## tensorflow model 
max_len=50
def tensorflow_based_model(): 
    inputs = Input(name='inputs',shape=[max_len])
    layer = Embedding(2000,50,input_length=max_len)(inputs) 
    layer = LSTM(64)(layer) 
    layer = Dense(256,name='FC1')(layer) 
    layer = Activation('relu')(layer) 
    layer = Dropout(0.5)(layer) 
    layer = Dense(1,name='out_layer')(layer) 
    layer = Activation('sigmoid')(layer) 
    model = Model(inputs=inputs,outputs=layer) 
    return model 
model = tensorflow_based_model() 
model.compile(loss='binary_crossentropy',optimizer='Adam',metrics=['accuracy'])
import joblib
from joblib import dump

    # dump the pipeline model
dump(model, filename="nn.joblib")
# from google.colab import files
# files.download('nn.joblib') 


X=tweets_df.absolute_tidy_tweets
y=target_variable
max_len = 50
tok = Tokenizer(num_words=2000)
tok.fit_on_texts(X)
sequences = tok.texts_to_sequences(X)
sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)
sequences_matrix.shape
X_train, X_test, Y_train, Y_test = train_test_split(sequences_matrix, y, test_size=0.3, random_state=2)
history=model.fit(X_train,Y_train,batch_size=80,epochs=20)
print('Training finished !!')
accr1 = model.evaluate(X_test,Y_test)
print(accr1[1])
# Comparison
import matplotlib.pyplot as plt
data = {'Gausian NB - BOW':24.1, 'Gausian NB - TF-IDF':24.6, 'Tensor flow - LSTM':29,
        }
courses = list(data.keys())
values = list(data.values())
  
fig = plt.figure(figsize = (10, 5))
 
# creating the bar plot
plt.bar(courses, values, color ='maroon',
        width = 0.4)
 
plt.xlabel("Models")
plt.ylabel("Accuracy")
plt.title("Accuracy of models")

plt.savefig("accuracy.pdf")
from google.colab import files
files.download('accuracy.pdf') 

